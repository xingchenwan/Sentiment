sdf
-. . -..- - / . -. - .-. -.--
with open(full_data_obj, 'rb') as f:
    all_data = pickle.load(f, fix_imports=True, encoding='bytes')
fix_fulldata(all_data)

start_date = all_data.start_date
end_date = all_data.end_date

price_time_series, vol_time_series = process_market_time_series(market_data_path, market_data_sheet,
                                                                start_date=pd.to_datetime(start_date),
                                                                end_date=pd.to_datetime(end_date))
-. . -..- - / . -. - .-. -.--
full_data_obj = 'full.date.20061020-20131120'
market_data_path = 'data/top_entities.xlsx'
market_data_sheet = 'Sheet3'

with open(full_data_obj, 'rb') as f:
    all_data = pickle.load(f, fix_imports=True, encoding='bytes')
fix_fulldata(all_data)

start_date = all_data.start_date
end_date = all_data.end_date

price_time_series, vol_time_series = process_market_time_series(market_data_path, market_data_sheet,
                                                                start_date=pd.to_datetime(start_date),
                                                                end_date=pd.to_datetime(end_date))

-. . -..- - / . -. - .-. -.--
import pydot
-. . -..- - / . -. - .-. -.--
import pydotplus
-. . -..- - / . -. - .-. -.--
import community
-. . -..- - / . -. - .-. -.--
partition = community.best_partition(G)
-. . -..- - / . -. - .-. -.--
size = float(len(set(partition.values())))
-. . -..- - / . -. - .-. -.--
pos = nx.spring_layout(G)
-. . -..- - / . -. - .-. -.--
pos = nx.nx_pydot.graphviz_layout(G)
-. . -..- - / . -. - .-. -.--
for com in set(partition.values()) :
count += 1.
list_nodes = [nodes for nodes in partition.keys()
if partition[nodes] == com]
nx.draw_networkx_nodes(G, pos, list_nodes, node_size = 20,
                                node_color = str(count / size))
-. . -..- - / . -. - .-. -.--
pos = nx.fruchterman_reingold_layout(G)
-. . -..- - / . -. - .-. -.--
count = 0.
-. . -..- - / . -. - .-. -.--
for com in set(partition.values()) :
    count += 1.
    list_nodes = [nodes for nodes in partition.keys()
    if partition[nodes] == com]
    nx.draw_networkx_nodes(G, pos, list_nodes, node_size = 20,
                                node_color = str(count / size))

-. . -..- - / . -. - .-. -.--
parition
-. . -..- - / . -. - .-. -.--
partition2 = community.generate_dendrogram(G)
-. . -..- - / . -. - .-. -.--
partition2
-. . -..- - / . -. - .-. -.--
partition
-. . -..- - / . -. - .-. -.--
pos = nx.spring_layout(partition)
-. . -..- - / . -. - .-. -.--
G = nx.erdos_renyi_graph(30, 0.05)
>>> #first compute the best partition
>>> partition = community.best_partition(G)
>>>  #drawing
>>> size = float(len(set(partition.values())))
>>> pos = nx.spring_layout(G)
>>> count = 0.
>>> for com in set(partition.values()) :
>>>     count += 1.
>>>     list_nodes = [nodes for nodes in partition.keys()
>>>                                 if partition[nodes] == com]
>>>     nx.draw_networkx_nodes(G, pos, list_nodes, node_size = 20,
                                node_color = str(count / size))
>>> nx.draw_networkx_edges(G,pos, alpha=0.5)
>>> plt.show()
-. . -..- - / . -. - .-. -.--
G = all_data.build_occurrence_network_graph(focus_dict=list(price_time_series.columns))
-. . -..- - / . -. - .-. -.--
>>> partition = community.best_partition(G)
>>>  #drawing
>>> size = float(len(set(partition.values())))
>>> pos = nx.spring_layout(G)
>>> count = 0.
>>> for com in set(partition.values()) :
>>>     count += 1.
>>>     list_nodes = [nodes for nodes in partition.keys()
>>>                                 if partition[nodes] == com]
>>>     nx.draw_networkx_nodes(G, pos, list_nodes, node_size = 20,
                                node_color = str(count / size))
>>> nx.draw_networkx_edges(G,pos, alpha=0.5)
>>> plt.show()
-. . -..- - / . -. - .-. -.--
full_data_obj = 'full.date.20061020-20131120'
market_data_path = 'data/top_entities.xlsx'
market_data_sheet = 'Sheet3'

with open(full_data_obj, 'rb') as f:
    all_data = pickle.load(f, fix_imports=True, encoding='bytes')
fix_fulldata(all_data)

start_date = all_data.start_date
end_date = all_data.end_date

price_time_series, vol_time_series = process_market_time_series(market_data_path, market_data_sheet,
                                                                start_date=pd.to_datetime(start_date),
                                                                end_date=pd.to_datetime(end_date))

# nx_graph = all_data.build_occurrence_network_graph(focus_dict=list(price_time_series.columns))


count_time_series, sentiment_avg_time_series, sentiment_sum_time_series = process_count_sentiment(all_data,
                                                                                                  include_list=list(
                                                                                                      price_time_series.columns))
res = correlator(price_time_series, vol_time_series, sentiment_avg_time_series, sentiment_sum_time_series,
                 count_time_series, plot=False)
-. . -..- - / . -. - .-. -.--
res['PriceSumSentimentCorr']
-. . -..- - / . -. - .-. -.--
res['PriceSumSentimentLag']
-. . -..- - / . -. - .-. -.--
test_array = res['PriceSumSentimentLag']
-. . -..- - / . -. - .-. -.--
test_array
-. . -..- - / . -. - .-. -.--
from main import stat_tests
-. . -..- - / . -. - .-. -.--
stat_tests(test_array)
-. . -..- - / . -. - .-. -.--
b"abcde"
-. . -..- - / . -. - .-. -.--
type(b'abcde')
-. . -..- - / . -. - .-. -.--
isinstance(b'abcde', bytes)
-. . -..- - / . -. - .-. -.--
isinstance(b'abcde', string)
-. . -..- - / . -. - .-. -.--
isinstance(b'abcde', str)
-. . -..- - / . -. - .-. -.--
full_data_obj = 'full.date.20061020-20131120'
    market_data_path = 'data/top_entities.xlsx'
    market_data_sheet = 'Sheet3'
    sector_path = "source/sector.csv"

    with open(full_data_obj, 'rb') as f:
        all_data = pickle.load(f, fix_imports=True, encoding='bytes')
    fix_fulldata(all_data)

    start_date = all_data.start_date
    end_date = all_data.end_date

    price_time_series, vol_time_series = process_market_time_series(market_data_path, market_data_sheet,
                                                                    start_date=pd.to_datetime(start_date),
                                                                    end_date=pd.to_datetime(end_date))

    # nx_graph = all_data.build_occurrence_network_graph(focus_dict=list(price_time_series.columns))

    count_time_series, sentiment_avg_time_series, sentiment_sum_time_series = process_count_sentiment(all_data,
                                                                    include_list=list(price_time_series.columns))
    res = correlator(price_time_series, vol_time_series, sentiment_avg_time_series, sentiment_sum_time_series,
                     count_time_series, plot=False, save_csv=True, display_summary_stats=False)

    #stat_tests(res['PriceCountCorr'])
    #stat_tests(res['PriceMedSentimentCorr'])
    #stat_tests(res['PriceSumSentimentCorr'])
    #stat_tests(res['VolCountCorr'])
    #stat_tests(res['SpotVolCorr'])

    sector = (get_sector(sector_path, 0, 2, res['Name']))
-. . -..- - / . -. - .-. -.--
full_data_obj = 'full.date.20061020-20131120'
market_data_path = 'data/top_entities.xlsx'
market_data_sheet = 'Sheet3'
sector_path = "source/sector.csv"

with open(full_data_obj, 'rb') as f:
    all_data = pickle.load(f, fix_imports=True, encoding='bytes')
fix_fulldata(all_data)

start_date = all_data.start_date
end_date = all_data.end_date

price_time_series, vol_time_series = process_market_time_series(market_data_path, market_data_sheet,
                                                                start_date=pd.to_datetime(start_date),
                                                                end_date=pd.to_datetime(end_date))

# nx_graph = all_data.build_occurrence_network_graph(focus_dict=list(price_time_series.columns))

count_time_series, sentiment_avg_time_series, sentiment_sum_time_series = process_count_sentiment(all_data,
                                                                                                  include_list=list(
                                                                                                      price_time_series.columns))
res = correlator(price_time_series, vol_time_series, sentiment_avg_time_series, sentiment_sum_time_series,
                 count_time_series, plot=False, save_csv=True, display_summary_stats=False)

# stat_tests(res['PriceCountCorr'])
# stat_tests(res['PriceMedSentimentCorr'])
# stat_tests(res['PriceSumSentimentCorr'])
# stat_tests(res['VolCountCorr'])
# stat_tests(res['SpotVolCorr'])

sector = (get_sector(sector_path, 0, 2, res['Name']))

-. . -..- - / . -. - .-. -.--
plot_scatter(res['PriceMedSentimentCorr'], res['Name'],
                 x_label='Stock Names', y_label='Correlation Coefficient', categories=sector)
-. . -..- - / . -. - .-. -.--
plot_scatter(res['PriceCountCorr'], res['Name'],
                 x_label='Stock Names', y_label='Correlation Coefficient', categories=sector)
-. . -..- - / . -. - .-. -.--
plot_scatter(res['VolCountCorr'], res['Name'],
                 x_label='Stock Names', y_label='Correlation Coefficient', categories=sector)
-. . -..- - / . -. - .-. -.--
plot_scatter(res['SpotVolCorr'], res['Name'],
                 x_label='Stock Names', y_label='Correlation Coefficient', categories=sector)
-. . -..- - / . -. - .-. -.--
stat_tests(res['PriceCountCorr'])
-. . -..- - / . -. - .-. -.--
stat_tests(res['SpotVolCorr'])
-. . -..- - / . -. - .-. -.--
stat_tests(res['PriceSumSentimentCorr'])
-. . -..- - / . -. - .-. -.--
pd.datetime("2013-3-3")
-. . -..- - / . -. - .-. -.--
pd.datetime(2013)
-. . -..- - / . -. - .-. -.--
pd.datetime(2013, 1, 2)
-. . -..- - / . -. - .-. -.--
datetime.datetime("2013,1,2")
-. . -..- - / . -. - .-. -.--
full_data_obj = 'full.date.20061020-20131120'
market_data_path = 'data/top_entities.xlsx'
market_data_sheet = 'Sheet3'
sector_path = "source/sector.csv"

with open(full_data_obj, 'rb') as f:
    all_data = pickle.load(f, fix_imports=True, encoding='bytes')
fix_fulldata(all_data)

G = create_sub_obj(all_data, datetime.date(2007, 1, 1), datetime.date(2008, 1, 1))
-. . -..- - / . -. - .-. -.--
with open(full_data_obj, 'rb') as f:
    all_data = pickle.load(f, fix_imports=True, encoding='bytes')
fix_fulldata(all_data)

G = create_sub_obj(all_data, datetime.date(2007, 1, 1), datetime.date(2008, 1, 1))
-. . -..- - / . -. - .-. -.--
G.start_date
-. . -..- - / . -. - .-. -.--
G.end_date
-. . -..- - / . -. - .-. -.--
G.entity_occur_dat
-. . -..- - / . -. - .-. -.--
G.entity_occur_day
-. . -..- - / . -. - .-. -.--
G.entity_occur_interval
-. . -..- - / . -. - .-. -.--
all_data.entity_occur_interval
-. . -..- - / . -. - .-. -.--
len(all_data.entity_sentiment_interval)
-. . -..- - / . -. - .-. -.--
len(G.entity_sentiment_interval)
-. . -..- - / . -. - .-. -.--
res
-. . -..- - / . -. - .-. -.--
start_date = all_data.start_date
end_date = all_data.start_date + datetime.timedelta(180)

price_time_series, vol_time_series = process_market_time_series(market_data_path, market_data_sheet,
                                                                start_date=pd.to_datetime(start_date),
                                                                end_date=pd.to_datetime(end_date))

# nx_graph = all_data.build_occurrence_network_graph(focus_dict=list(price_time_series.columns))

count_time_series, sentiment_avg_time_series, sentiment_sum_time_series = process_count_sentiment(all_data,
                                                                                                  include_list=list(
                                                                                                      price_time_series.columns))
res = correlator(price_time_series, vol_time_series, sentiment_avg_time_series, sentiment_sum_time_series,
                 count_time_series, plot=False, save_csv=True, display_summary_stats=False)
-. . -..- - / . -. - .-. -.--
import *
-. . -..- - / . -. - .-. -.--
full_data_obj = 'full.date.20061020-20131120'
    market_data_path = 'data/top_entities.xlsx'
    market_data_sheet = 'Sheet3'
    sector_path = "source/sector.csv"

    with open(full_data_obj, 'rb') as f:
        all_data = pickle.load(f, fix_imports=True, encoding='bytes')
    utilities.fix_fulldata(all_data)

    start_date = datetime.date(2008, 1, 1)
    end_date = datetime.date(2008, 12, 31)
    price, _, log_return, vol = processor.process_market_time_series(market_data_path, market_data_sheet,
                                                      start_date=pd.to_datetime(start_date),
                                                      end_date=pd.to_datetime(end_date))

    count, med_sentiment, sum_sentiment = processor.process_count_sentiment(all_data,
                                                                            start_date=pd.to_datetime(start_date),
                                                                            end_date=pd.to_datetime(end_date),
                                                                            focus_iterable=list(price.columns))

    res = analyser.correlate(log_return, vol, med_sentiment, sum_sentiment,
                             count, max_lag=0, plot=False, save_csv=False, display_summary_stats=False)
-. . -..- - / . -. - .-. -.--
full_data_obj = 'full.date.20061020-20131120'
market_data_path = 'data/top_entities.xlsx'
market_data_sheet = 'Sheet3'
sector_path = "source/sector.csv"

with open(full_data_obj, 'rb') as f:
    all_data = pickle.load(f, fix_imports=True, encoding='bytes')
utilities.fix_fulldata(all_data)

start_date = datetime.date(2008, 1, 1)
end_date = datetime.date(2008, 12, 31)
price, _, log_return, vol = processor.process_market_time_series(market_data_path, market_data_sheet,
                                                                 start_date=pd.to_datetime(start_date),
                                                                 end_date=pd.to_datetime(end_date))

count, med_sentiment, sum_sentiment = processor.process_count_sentiment(all_data,
                                                                        start_date=pd.to_datetime(start_date),
                                                                        end_date=pd.to_datetime(end_date),
                                                                        focus_iterable=list(price.columns))

res = analyser.correlate(log_return, vol, med_sentiment, sum_sentiment,
                         count, max_lag=0, plot=False, save_csv=False, display_summary_stats=False)

-. . -..- - / . -. - .-. -.--
dynamic_group = grouper.get_dynamic_grouping(res['Name'], all_data, start_date, end_date)

-. . -..- - / . -. - .-. -.--
visualiser.plot_scatter(res['PriceSumSentimentCorr'], res['Name'],
                        x_label='Stock Names', y_label='Correlation Coefficient', categories=dynamic_group)

-. . -..- - / . -. - .-. -.--
visualiser.plot_scatter(res['PriceMedSentimentCorr'], res['Name'],
                        x_label='Stock Names', y_label='Correlation Coefficient', categories=dynamic_group)

-. . -..- - / . -. - .-. -.--
res = analyser.correlate(log_return, vol, med_sentiment, sum_sentiment,
                         count, max_lag=2, plot=False, save_csv=False, display_summary_stats=False)

-. . -..- - / . -. - .-. -.--
res = analyser.correlate(log_return, vol, med_sentiment, sum_sentiment,
                         count, max_lag=2, plot=True, save_csv=False, display_summary_stats=False)
-. . -..- - / . -. - .-. -.--
res = analyser.correlate(log_return, vol, med_sentiment, sum_sentiment,
                         count, max_lag=4, plot=False, save_csv=False, display_summary_stats=False)
-. . -..- - / . -. - .-. -.--
visualiser.plot_scatter(res['PriceCountCorr'], res['Name'],
                        x_label='Stock Names', y_label='Correlation Coefficient', categories=dynamic_group)
-. . -..- - / . -. - .-. -.--
visualiser.plot_scatter(res['PriceMedSentimentCorr'], res['Name'],
                        x_label='Stock Names', y_label='Correlation Coefficient', categories=dynamic_group)
-. . -..- - / . -. - .-. -.--
visualiser.plot_scatter(res['PriceSumSentimentCorr'], res['Name'],
                        x_label='Stock Names', y_label='Correlation Coefficient', categories=dynamic_group)
-. . -..- - / . -. - .-. -.--
start_date = datetime.date(2004, 1, 1)
    end_date = datetime.date(2018, 6, 30)
    price, _, log_return, vol = processor.process_market_time_series(market_data_path, market_data_sheet,
                                                      start_date=pd.to_datetime(start_date),
                                                      end_date=pd.to_datetime(end_date))

    count, med_sentiment, sum_sentiment = processor.process_count_sentiment(all_data,
                                                                            start_date=pd.to_datetime(start_date),
                                                                            end_date=pd.to_datetime(end_date),
                                                                            focus_iterable=list(price.columns))

    res = analyser.correlate(price, vol, med_sentiment, sum_sentiment,
                             count, max_lag=4, plot=False, save_csv=False, display_summary_stats=False)
    #stat_tests(res['PriceCountCorr'])
    #stat_tests(res['PriceMedSentimentCorr'])
    # analyser.stat_tests(res['PriceSumSentimentCorr'])
    # analyser.stat_tests(res['VolCountCorr'])
    #stat_tests(res['SpotVolCorr'])
    sector = (grouper.get_sector_grouping(res['Name'],sector_path, 0, 2, ))
-. . -..- - / . -. - .-. -.--
start_date = datetime.date(2004, 1, 1)
end_date = datetime.date(2018, 6, 30)
price, _, log_return, vol = processor.process_market_time_series(market_data_path, market_data_sheet,
                                                                 start_date=pd.to_datetime(start_date),
                                                                 end_date=pd.to_datetime(end_date))

count, med_sentiment, sum_sentiment = processor.process_count_sentiment(all_data,
                                                                        start_date=pd.to_datetime(start_date),
                                                                        end_date=pd.to_datetime(end_date),
                                                                        focus_iterable=list(price.columns))

res = analyser.correlate(price, vol, med_sentiment, sum_sentiment,
                         count, max_lag=4, plot=False, save_csv=False, display_summary_stats=False)
# stat_tests(res['PriceCountCorr'])
# stat_tests(res['PriceMedSentimentCorr'])
# analyser.stat_tests(res['PriceSumSentimentCorr'])
# analyser.stat_tests(res['VolCountCorr'])
# stat_tests(res['SpotVolCorr'])
sector = (grouper.get_sector_grouping(res['Name'], sector_path, 0, 2, ))
-. . -..- - / . -. - .-. -.--
visualiser.plot_scatter(res['PriceSumSentimentCorr'], res['Name'],
                 x_label='Stock Names', y_label='Correlation Coefficient', categories=sector)

-. . -..- - / . -. - .-. -.--
visualiser.plot_scatter(res['PriceCountCorr'], res['Name'],
                 x_label='Stock Names', y_label='Correlation Coefficient', categories=sector)


-. . -..- - / . -. - .-. -.--
visualiser.plot_scatter(res['PriceMedSentimentCorr'], res['Name'],
                 x_label='Stock Names', y_label='Correlation Coefficient', categories=sector)


-. . -..- - / . -. - .-. -.--
visualiser.plot_scatter(res['VolCountCorr'], res['Name'],
                 x_label='Stock Names', y_label='Correlation Coefficient', categories=sector)



-. . -..- - / . -. - .-. -.--
full_data_obj = 'full.date.20061020-20131120'
market_data_path = 'data/top_entities.xlsx'
market_data_sheet = 'Sheet3'
sector_path = "source/sector.csv"

with open(full_data_obj, 'rb') as f:
    all_data = pickle.load(f, fix_imports=True, encoding='bytes')
utilities.fix_fulldata(all_data)

start_date = datetime.date(2004, 1, 1)
end_date = datetime.date(2018, 6, 30)
price, _, log_return, vol = processor.process_market_time_series(market_data_path, market_data_sheet,
                                                                 start_date=pd.to_datetime(start_date),
                                                                 end_date=pd.to_datetime(end_date))

-. . -..- - / . -. - .-. -.--
count, med_sentiment, sum_sentiment = processor.process_count_sentiment(all_data,
                                                                        start_date=pd.to_datetime(start_date),
                                                                        end_date=pd.to_datetime(end_date),
                                                                        focus_iterable=list(price.columns))

res = analyser.correlate(price, vol, med_sentiment, sum_sentiment,
                         count, max_lag=4, plot=False, save_csv=False, display_summary_stats=False)
-. . -..- - / . -. - .-. -.--
visualiser.plot_single_name('LEHMQ', price, log_return, vol, count, med_sentiment,
                            arg_names=["px", "log_return", "vol"])

-. . -..- - / . -. - .-. -.--
print(vol)
-. . -..- - / . -. - .-. -.--
type(count.index)
-. . -..- - / . -. - .-. -.--
price.index
-. . -..- - / . -. - .-. -.--
count.index
-. . -..- - / . -. - .-. -.--
count.index[0]
-. . -..- - / . -. - .-. -.--
price.index[0]
-. . -..- - / . -. - .-. -.--
type(price.index[0])
-. . -..- - / . -. - .-. -.--
full_data_obj = 'full.date.20061020-20131120'
market_data_path = 'data/top_entities.xlsx'
market_data_sheet = 'Sheet3'
sector_path = "source/sector.csv"

with open(full_data_obj, 'rb') as f:
    all_data = pickle.load(f, fix_imports=True, encoding='bytes')
utilities.fix_fulldata(all_data)

start_date = datetime.date(2004, 1, 1)
end_date = datetime.date(2018, 6, 30)
price, _, log_return, vol = processor.process_market_time_series(market_data_path, market_data_sheet,
                                                                 start_date=pd.to_datetime(start_date),
                                                                 end_date=pd.to_datetime(end_date))

count, med_sentiment, sum_sentiment = processor.process_count_sentiment(all_data,
                                                                        start_date=pd.to_datetime(start_date),
                                                                        end_date=pd.to_datetime(end_date),
                                                                        focus_iterable=list(price.columns))

res = analyser.correlate(price, vol, med_sentiment, sum_sentiment,
                         count, max_lag=4, plot=False, save_csv=False, display_summary_stats=False)
-. . -..- - / . -. - .-. -.--
visualiser.plot_single_name('LEHMQ', price, log_return, vol, count, med_sentiment,
                            arg_names=["px", "log_return", "vol", "count", "med_sentiment"])
-. . -..- - / . -. - .-. -.--
a = [1,2,3,4,5,5,4,3,2,2,1]
-. . -..- - / . -. - .-. -.--
a = pd.Series(a)
-. . -..- - / . -. - .-. -.--
a.rolling(window=2)
-. . -..- - / . -. - .-. -.--
a[1:3]
-. . -..- - / . -. - .-. -.--
a[1:4]
-. . -..- - / . -. - .-. -.--
a[2:4]
-. . -..- - / . -. - .-. -.--
full_data_obj = 'full.date.20061020-20131120'
market_data_path = 'data/top_entities.xlsx'
market_data_sheet = 'Sheet3'
sector_path = "source/sector.csv"

with open(full_data_obj, 'rb') as f:
    all_data = pickle.load(f, fix_imports=True, encoding='bytes')
utilities.fix_fulldata(all_data)
-. . -..- - / . -. - .-. -.--
for day in all_data.days:
    print(day.entity_occur_day['MS'])
    print(day.entity_sentiment_day['MS'])
    
-. . -..- - / . -. - .-. -.--
for day in all_data.days:
    print(day.entity_occur_day[b'MS'])
    print(day.entity_sentiment_day[b'MS'])
    
-. . -..- - / . -. - .-. -.--
for day in all_data.days:
    print(day.entity_occur_day)
    
-. . -..- - / . -. - .-. -.--
for day in all_data.days:
    try:
        print(day.entity_occur_day['MS'])
    except KeyError:
        pass

-. . -..- - / . -. - .-. -.--
for day in all_data.days:
    try:
        print(day.entity_sentiment_day['MS'])
    except KeyError:
        pass

-. . -..- - / . -. - .-. -.--
for day in all_data.days:
    print(day.date)

-. . -..- - / . -. - .-. -.--
all_data.entity_sentiment_interval['MS']
-. . -..- - / . -. - .-. -.--
[i for i in all_data.days if 'MS' in i.entity_occur_day]
-. . -..- - / . -. - .-. -.--
len([i for i in all_data.days if 'MS' in i.entity_occur_day])
-. . -..- - / . -. - .-. -.--
len([i for i in all_data.days if 'GS' in i.entity_occur_day])
-. . -..- - / . -. - .-. -.--
len([i for i in all_data.days if 'MS' in i.entity_occur_day and i.date>datetime.date(2007, 1, 1) and i.date<datetime.date(2009, 6, 30)])
-. . -..- - / . -. - .-. -.--
start_date = datetime.date(2007, 1, 1)
end_date = datetime.date(2009, 6, 30)
price, _, log_return, vol = processor.process_market_time_series(market_data_path, market_data_sheet,
                                                                 start_date=pd.to_datetime(start_date),
                                                                 end_date=pd.to_datetime(end_date))
count, med_sentiment, sum_sentiment = processor.process_count_sentiment(all_data,
                                                                        start_date=pd.to_datetime(start_date),
                                                                        end_date=pd.to_datetime(end_date),
                                                                        focus_iterable=list(price.columns))
-. . -..- - / . -. - .-. -.--
count['MS'].sum()
-. . -..- - / . -. - .-. -.--
len([i for i in all_data.days if 'GS' in i.entity_occur_day and i.date>datetime.date(2007, 1, 1) and i.date<datetime.date(2009, 6, 30)])
-. . -..- - / . -. - .-. -.--
count['GS'].sum()
-. . -..- - / . -. - .-. -.--
a = [False, False, False, True, True]
-. . -..- - / . -. - .-. -.--
a = np.asarray(a)
-. . -..- - / . -. - .-. -.--
a == False
-. . -..- - / . -. - .-. -.--
a[3]=False
-. . -..- - / . -. - .-. -.--
a[4]=False
-. . -..- - / . -. - .-. -.--
a is False
-. . -..- - / . -. - .-. -.--
(a == False).all()
-. . -..- - / . -. - .-. -.--
a = np([0,0,1,0,0])
-. . -..- - / . -. - .-. -.--
a = np.array([0,0,1,0,0])
-. . -..- - / . -. - .-. -.--
b = np.array([0,0,0,1,0])
-. . -..- - / . -. - .-. -.--
np.correlate(a,b,"full")
-. . -..- - / . -. - .-. -.--
np.correlate(b, a, 'full')
-. . -..- - / . -. - .-. -.--
from main improt *
-. . -..- - / . -. - .-. -.--
full_data_obj = 'full.date.20061020-20131120'
    market_data_path = 'data/top_entities.xlsx'
    market_data_sheet = 'Sheet3'
    sector_path = "source/sector.csv"

    with open(full_data_obj, 'rb') as f:
        all_data = pickle.load(f, fix_imports=True, encoding='bytes')
    utilities.fix_fulldata(all_data)

    start_date = datetime.date(2007, 1, 1)
    end_date = datetime.date(2018, 1, 1)


    price, _, log_return, vol = processor.process_market_time_series(market_data_path, market_data_sheet,
                                                      start_date=pd.to_datetime(start_date),
                                                      end_date=pd.to_datetime(end_date),
                                                      )
    all = list(price.columns)

    sector = (grouper.get_sector_grouping(all, sector_path, 0, 2, ))
-. . -..- - / . -. - .-. -.--
full_data_obj = 'full.date.20061020-20131120'
market_data_path = 'data/top_entities.xlsx'
market_data_sheet = 'Sheet3'
sector_path = "source/sector.csv"

with open(full_data_obj, 'rb') as f:
    all_data = pickle.load(f, fix_imports=True, encoding='bytes')
utilities.fix_fulldata(all_data)

start_date = datetime.date(2007, 1, 1)
end_date = datetime.date(2018, 1, 1)

price, _, log_return, vol = processor.process_market_time_series(market_data_path, market_data_sheet,
                                                                 start_date=pd.to_datetime(start_date),
                                                                 end_date=pd.to_datetime(end_date),
                                                                 )
all = list(price.columns)

sector = (grouper.get_sector_grouping(all, sector_path, 0, 2, ))
-. . -..- - / . -. - .-. -.--
dynamic = grouper.get_dynamic_grouping(all, all_data, start_date=datetime.date(2007, 7, 1),
                                           end_date=datetime.date(2007, 12, 31))
-. . -..- - / . -. - .-. -.--
a = [1, 2, 3]
-. . -..- - / . -. - .-. -.--
pd.Series(a)
-. . -..- - / . -. - .-. -.--
all_data
-. . -..- - / . -. - .-. -.--
all_data.days
-. . -..- - / . -. - .-. -.--
all_data.dates.day
-. . -..- - / . -. - .-. -.--
all_data.day[0].date
-. . -..- - / . -. - .-. -.--
all_data.days[0].date
-. . -..- - / . -. - .-. -.--
all_data.days[1].date
-. . -..- - / . -. - .-. -.--
all_data.days[2].date
-. . -..- - / . -. - .-. -.--
a = pd.Series([1, 2, 3, 4, 5])
-. . -..- - / . -. - .-. -.--
a.shift(1)
-. . -..- - / . -. - .-. -.--
a.shift(-1)
-. . -..- - / . -. - .-. -.--
import numpy as np
-. . -..- - / . -. - .-. -.--
np.log(a.shift(-1)) - np.log(a)
-. . -..- - / . -. - .-. -.--
np.log(a) - np.log(a.shift(1))
-. . -..- - / . -. - .-. -.--
a = pd.Series([0,1,2,3,1,2,3])
-. . -..- - / . -. - .-. -.--
a.rolling()
-. . -..- - / . -. - .-. -.--
a.rolling(3).mean()
-. . -..- - / . -. - .-. -.--
from pandas.tseries.offsets import BDay

-. . -..- - / . -. - .-. -.--
import datetime
-. . -..- - / . -. - .-. -.--
today = datetime(2018, 9, 14)
-. . -..- - / . -. - .-. -.--
today = datetime.date(2018, 9, 14)
-. . -..- - / . -. - .-. -.--
today + BDay(0)
-. . -..- - / . -. - .-. -.--
today + BDay(2)
-. . -..- - / . -. - .-. -.--
today + BDay(-1)
-. . -..- - / . -. - .-. -.--
a = pd.Series([0, 1, 2, 4, 8, 4, 2, 1])
-. . -..- - / . -. - .-. -.--
a.plot()
-. . -..- - / . -. - .-. -.--
a.ewm(alpha=0.1).mean()
-. . -..- - / . -. - .-. -.--
a.ewm(alpha=0.9).mean().plot()
-. . -..- - / . -. - .-. -.--
a.ewm(alpha=0.1).mean().plot()
-. . -..- - / . -. - .-. -.--
a = pd.DataFrame([[1,2,3],[1,2,3],[1,2,3]])
-. . -..- - / . -. - .-. -.--
from scipy import stats
-. . -..- - / . -. - .-. -.--
stats.ttest_1samp(a, popmean=0)
-. . -..- - / . -. - .-. -.--
stats.ttest_1samp(a, popmean=0)[pvalue]
-. . -..- - / . -. - .-. -.--
stats.ttest_1samp(a, popmean=0)['pvalue']
-. . -..- - / . -. - .-. -.--
a = pd.DataFrame([[1,2,1],[1,2,0],[1,2,-1]])
-. . -..- - / . -. - .-. -.--
stats.ttest_1samp(a, popmean=0)[1]
-. . -..- - / . -. - .-. -.--
range(-5, 5)
-. . -..- - / . -. - .-. -.--
list(range(-5, 5))
-. . -..- - / . -. - .-. -.--
full_data_obj = 'full.date.20061020-20131120'
market_data_path = 'data/top_entities.xlsx'
market_data_sheet = 'Sheet3'
sector_path = "source/sector.csv"

with open(full_data_obj, 'rb') as f:
    all_data = pickle.load(f, fix_imports=True, encoding='bytes')
utilities.fix_fulldata(all_data)

start_date = datetime.date(2006, 10, 1)
end_date = datetime.date(2013, 12, 31)

sub_all_data = utilities.create_sub_obj(all_data, start_date, end_date)
price, _, log_return, vol = processor.process_market_time_series(market_data_path, market_data_sheet,
                                                                 start_date=pd.to_datetime(start_date),
                                                                 end_date=pd.to_datetime(end_date),
                                                                 )

# dynamic = grouper.get_dynamic_grouping(all, sub_all_data)
# visualiser.plot_network(sub_all_data, names=all, categories=dynamic)
# plt.show()


this_name = 'GS'
all_names = list(price.columns)
sector = grouper.get_sector_grouping(all_names, 'source/sector.csv', 0, 2)
this_sector = grouper.get_sector_peer(sector, this_name)

count, med_sentiment, sum_sentiment = processor.process_count_sentiment(all_data,
                                                                        start_date=pd.to_datetime(start_date),
                                                                        end_date=pd.to_datetime(end_date),
                                                                        focus_iterable=all_names,
                                                                        rolling=True)

-. . -..- - / . -. - .-. -.--
pos, neg = analyser.event_analyser(this_sector, log_return, sum_sentiment,)
-. . -..- - / . -. - .-. -.--
pos.mean()
-. . -..- - / . -. - .-. -.--
pos.mean
-. . -..- - / . -. - .-. -.--
pos.mean 
-. . -..- - / . -. - .-. -.--
pos.iloc[1, :]
-. . -..- - / . -. - .-. -.--
neg.iloc[i, :].plot()
-. . -..- - / . -. - .-. -.--
this_sector = grouper.get_sector_peer(sector, 'PFE')
-. . -..- - / . -. - .-. -.--
pos.iloc[1,:].plot()
-. . -..- - / . -. - .-. -.--
pos.iloc[0,:]
-. . -..- - / . -. - .-. -.--
neg.iloc[0,:]
-. . -..- - / . -. - .-. -.--
this_sector = grouper.get_sector_peer(sector, 'BP')
-. . -..- - / . -. - .-. -.--
this_sector = grouper.get_sector_peer(sector, 'VWGN')
-. . -..- - / . -. - .-. -.--
this_sector = grouper.get_sector_peer(sector, 'BMWG')
-. . -..- - / . -. - .-. -.--
pos.iloc[0, 0]
-. . -..- - / . -. - .-. -.--
neg.iloc[0, 0]
-. . -..- - / . -. - .-. -.--
this_sector = grouper.get_sector_peer(sector, 'RIO')
-. . -..- - / . -. - .-. -.--
this_sector = grouper.get_sector_peer(sector, 'AA')
-. . -..- - / . -. - .-. -.--
this_sector = grouper.get_sector_peer(sector, 'NISSAN')
-. . -..- - / . -. - .-. -.--
this_sector = grouper.get_sector_peer(sector, 'MCD')
-. . -..- - / . -. - .-. -.--
len(this_sector)
-. . -..- - / . -. - .-. -.--
pos.iloc[i, :].plot()
-. . -..- - / . -. - .-. -.--
this_sector = grouper.get_sector_peer(sector, 'AAPL')
-. . -..- - / . -. - .-. -.--
pos, neg = analyser.event_analyser(this_sector, log_return, sum_sentiment, max_lag=10)
-. . -..- - / . -. - .-. -.--
pos, neg = analyser.event_analyser(this_sector, log_return, sum_sentiment, max_lag=5)
-. . -..- - / . -. - .-. -.--
pos
-. . -..- - / . -. - .-. -.--
neg
-. . -..- - / . -. - .-. -.--
pos, neg = analyser.event_analyser(this_sector, log_return, sum_sentiment, max_lag=5, save_csv=True)
-. . -..- - / . -. - .-. -.--
count, med_sentiment, sum_sentiment = processor.process_count_sentiment(all_data,
                                                                            start_date=pd.to_datetime(start_date),
                                                                            end_date=pd.to_datetime(end_date),
                                                                            focus_iterable=all_names,
                                                                            rolling=True,
                                                                            rolling_smoothing_factor=0.7)
-. . -..- - / . -. - .-. -.--
pos, neg = analyser.event_analyser(this_sector, log_return, sum_sentiment, max_lag=5, save_csv=False)
-. . -..- - / . -. - .-. -.--
price, daily_rtn, log_return, vol = processor.process_market_time_series(market_data_path, market_data_sheet,
                                                                         start_date=pd.to_datetime(start_date),
                                                                         end_date=pd.to_datetime(end_date),
                                                                         )

# dynamic = grouper.get_dynamic_grouping(all, sub_all_data)
# visualiser.plot_network(sub_all_data, names=all, categories=dynamic)
# plt.show()


this_name = 'GS'
all_names = list(price.columns)
sector = grouper.get_sector_grouping(all_names, 'source/sector.csv', 0, 2)
this_sector = grouper.get_sector_peer(sector, this_name)

count, med_sentiment, sum_sentiment = processor.process_count_sentiment(all_data,
                                                                        start_date=pd.to_datetime(start_date),
                                                                        end_date=pd.to_datetime(end_date),
                                                                        focus_iterable=all_names,
                                                                        rolling=True,
                                                                        rolling_smoothing_factor=0.8)

-. . -..- - / . -. - .-. -.--
analyser.event_analyser(this_sector, daily_rtn, sum_sentiment,)
-. . -..- - / . -. - .-. -.--
pos, neg = analyser.event_analyser(this_sector, daily_rtn, sum_sentiment,)
-. . -..- - / . -. - .-. -.--
pos.iloc[1, :].plot()
-. . -..- - / . -. - .-. -.--
neg.ioc[1, :].plot()
-. . -..- - / . -. - .-. -.--
neg.iloc[1, :].plot()
-. . -..- - / . -. - .-. -.--
import pandas as pd
-. . -..- - / . -. - .-. -.--
a = pd.DataFrame({a: 1, b: 2, c: 3})
-. . -..- - / . -. - .-. -.--
pos['mean']
-. . -..- - / . -. - .-. -.--
pos.loc[:]
-. . -..- - / . -. - .-. -.--
pos.loc['mean', :]
-. . -..- - / . -. - .-. -.--
full_data_obj = 'full.date.20061020-20131120'
market_data_path = 'data/top_entities.xlsx'
market_data_sheet = 'Sheet3'
sector_path = "source/sector.csv"

with open(full_data_obj, 'rb') as f:
    all_data = pickle.load(f, fix_imports=True, encoding='bytes')
utilities.fix_fulldata(all_data)

start_date = datetime.date(2011, 10, 1)
end_date = datetime.date(2013, 12, 31)

sub_all_data = utilities.create_sub_obj(all_data, start_date, end_date)
price, daily_rtn, log_return, vol = processor.process_market_time_series(market_data_path, market_data_sheet,
                                                                         start_date=pd.to_datetime(start_date),
                                                                         end_date=pd.to_datetime(end_date),
                                                                         )

# dynamic = grouper.get_dynamic_grouping(all, sub_all_data)
# visualiser.plot_network(sub_all_data, names=all, categories=dynamic)
# plt.show()


this_name = 'GPS'
all_names = list(price.columns)
sector = grouper.get_sector_grouping(all_names, 'source/sector.csv', 0, 2)
this_sector = grouper.get_sector_peer(sector, this_name)

count, med_sentiment, sum_sentiment = processor.process_count_sentiment(all_data,
                                                                        start_date=pd.to_datetime(start_date),
                                                                        end_date=pd.to_datetime(end_date),
                                                                        focus_iterable=all_names,
                                                                        rolling=True,
                                                                        rolling_smoothing_factor=0.7)

pos, neg = analyser.event_analyser(['GPS'], daily_rtn, sum_sentiment, max_lag=5, rolling_window=90,
                                   detection_threshold=2.5)
-. . -..- - / . -. - .-. -.--
visualiser.plot_events(pos, descp='Positive Events')

-. . -..- - / . -. - .-. -.--
visualiser.plot_events(pos, descp=['Positive Events'])

-. . -..- - / . -. - .-. -.--
a = np.array([1,2,3,4,5])
-. . -..- - / . -. - .-. -.--
a[[1,2]]
-. . -..- - / . -. - .-. -.--
a[1,2]
-. . -..- - / . -. - .-. -.--
a = [1, 2, 3, 4, 5]
-. . -..- - / . -. - .-. -.--
a += 1
-. . -..- - / . -. - .-. -.--
633+1246
-. . -..- - / . -. - .-. -.--
316+685
-. . -..- - / . -. - .-. -.--
384+417
-. . -..- - / . -. - .-. -.--
46+235
-. . -..- - / . -. - .-. -.--
376+579
-. . -..- - / . -. - .-. -.--
from main import *
-. . -..- - / . -. - .-. -.--
full_data_obj = 'full.date.20061020-20131120'
market_data_path = 'data/top_entities.xlsx'
market_data_sheet = 'Sheet3'
sector_path = "source/sector.csv"

with open(full_data_obj, 'rb') as f:
    all_data = pickle.load(f, fix_imports=True, encoding='bytes')
utilities.fix_fulldata(all_data)

start_date = datetime.date(2007, 1, 1)
end_date = datetime.date(2008, 1, 1)

sub_all_data = utilities.create_sub_obj(all_data, start_date, end_date)
price, daily_rtn, log_return, vol = processor.process_market_time_series(market_data_path, market_data_sheet,
                                                                         start_date=pd.to_datetime(start_date),
                                                                         end_date=pd.to_datetime(end_date),
                                                                         )

this_name = 'MS'
all_names = list(price.columns)
sector = grouper.get_sector_grouping(all_names, 'source/sector.csv', 0, 2)
this_sector = grouper.get_sector_peer(sector, this_name)
dynamic = grouper.get_dynamic_grouping(all_names, sub_all_data)

-. . -..- - / . -. - .-. -.--
visualiser.plot_network(sub_all_data, names=all_names, categories=dynamic, group_by='centrality')

-. . -..- - / . -. - .-. -.--
sub_all_data
-. . -..- - / . -. - .-. -.--
sub_all_data.build_occurrence_network_graph(all_names)
-. . -..- - / . -. - .-. -.--
sub_all_data.build_occurrence_network_graph(focus_iterable=all_names)
-. . -..- - / . -. - .-. -.--
G = sub_all_data.build_occurrence_network_graph(focus_iterable=all_names)
-. . -..- - / . -. - .-. -.--
centrality = list(nx.katz_centrality(G, weight='weight'))
-. . -..- - / . -. - .-. -.--
centrality = nx.katz_centrality(G, weight='weight')
-. . -..- - / . -. - .-. -.--
centrality
-. . -..- - / . -. - .-. -.--
sorted_centrality = sorted(centrality.items(), key=lambda kv: kv[1])
-. . -..- - / . -. - .-. -.--
sorted_centrality
-. . -..- - / . -. - .-. -.--
G
-. . -..- - / . -. - .-. -.--
G['MS']
-. . -..- - / . -. - .-. -.--
sorted(G['MS'].items(), key=lambda v: v[1][1])
-. . -..- - / . -. - .-. -.--
sorted(G['MS'].items(), key=lambda v: v[1]['weight'])
-. . -..- - / . -. - .-. -.--
sorted(G['MS'].items(), key=lambda v: -v[1]['weight'])
-. . -..- - / . -. - .-. -.--
A = sorted(G['MS'].items(), key=lambda v: -v[1]['weight'])
-. . -..- - / . -. - .-. -.--
A[:2]
-. . -..- - / . -. - .-. -.--
A
-. . -..- - / . -. - .-. -.--
A[0]
-. . -..- - / . -. - .-. -.--
A[0][1]
-. . -..- - / . -. - .-. -.--
A[0][1]['weight']
-. . -..- - / . -. - .-. -.--
a = {}
-. . -..- - / . -. - .-. -.--
a['e'] = 4
-. . -..- - / . -. - .-. -.--
a